---
title: "Extrinsic Evaluation of Machine Translation Metrics"
collection: publications
venue: ACL 2023
permalink: /publications/2022-12-ext-mt
paperurl: https://arxiv.org/abs/2212.10297
---
Authors: Nikita Moghe, Tom Sherborne, Mark Steedman, Alexandra Birch


[paper](https://www.aclweb.org/anthology/D18-1255/) [data](https://huggingface.co/datasets/uoe-nlp/extrinsic_mt_eval) [slides](https://docs.google.com/presentation/d/1Yv3eBRdedSC_nUPa2toHguLqHMfj60eMTM6G7uUWB94/edit?usp=sharing) [poster](https://docs.google.com/presentation/d/1Bp8ee7QFr8LT1PVqrg94t9cnjQNWaSGP25CzRRhad8E/edit?usp=sharing) 



Automatic machine translation (MT) metrics are widely used to distinguish the translation qualities of machine translation systems across relatively large test sets (system-level evaluation). However, it is unclear if automatic metrics are reliable at distinguishing good translations from bad translations at the sentence level (segment-level evaluation). In this paper, we investigate how useful MT metrics are at detecting the success of a machine translation component when placed in a larger platform with a downstream task. We evaluate the segment-level performance of the most widely used MT metrics (chrF, COMET, BERTScore, etc.) on three downstream cross-lingual tasks (dialogue state tracking, question answering, and semantic parsing). For each task, we only have access to a monolingual task-specific model. We calculate the correlation between the metric's ability to predict a good/bad translation with the success/failure on the final task for the Translate-Test setup. Our experiments demonstrate that all metrics exhibit negligible correlation with the extrinsic evaluation of the downstream outcomes. We also find that the scores provided by neural metrics are not interpretable mostly because of undefined ranges. Our analysis suggests that future MT metrics be designed to produce error labels rather than scores to facilitate extrinsic evaluation.
